#-------------------------------------- Chnage Log -------------------------------------------
__author__ = "Anup Kumar"
__copyright__ = "Copyright 2017, KAPSARC EIM Project"
__credits__ = ["Anup Kumar"]
__license__ = "Prop"
__version__ = "1.0.0"
__maintainer__ = "Anup Kumar"
__email__ = "anup.kumar@kapsarc.org"
__status__ = "Production"
#------------------------------------ Change Details ------------------------------------------
# 11/22/2018 - Anup Kumar - Download Knoema file to inputs folder
# 07/09/2019 - Anup Kumar - Added capabilities to load both the delta and new datasets
# 07/09/2019 - Anup Kumar - Multiprocessing
# 07/17/2019 - Anup Kumar - Integrated with Airflow
# 07/18/2019 - Anup Kumar - Update the AF variables and KDS meta, fixed encoding issues nad publishing
# 07/21/2019 - Anup Kumar - Fixed bugs
# 09/10/2019 - Anup Kumar - Updated the FTP to AWS
#----------------------------------------------------------------------------------------------

import os, platform, requests, json, time, re, base64, hashlib, hmac, datetime
from urllib.parse import urlparse, urljoin
from requests.auth import HTTPBasicAuth
import pandas as pd
import airflow
from airflow import DAG
from airflow.models import Variable
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.hooks.postgres_hook import PostgresHook
from airflow.hooks.S3_hook import S3Hook
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.subdag_operator import SubDagOperator

META_URL = "https://kapsarc.knoema.com/api/1.0/meta/dataset/"
DATA_URL = "https://kapsarc.knoema.com/api/1.1/data/details"
DIR_PATH = os.path.abspath(os.path.dirname(__file__))
FREQ_MAPPING = {'M': 'Monthly', 'A': 'Annual', 'D': 'Daily', 'Q': 'Quarterly', 'W': 'Weekly'}

def get_raw_data(row):
    url = DATA_URL
    dataset_id = row['knoema_id'].strip()
    last_load_date = row['last_load_date']
    next_load_date = row['next_load_date']

    if (last_load_date == None) or ((datetime.datetime(next_load_date.year, next_load_date.month, next_load_date.day)-datetime.datetime.utcfromtimestamp(0)).total_seconds()*1000 <= (datetime.datetime(datetime.date.today().year, datetime.date.today().month, datetime.date.today().day)-datetime.datetime.utcfromtimestamp(0)).total_seconds()*1000):
        print ("Processing dataset id - " + dataset_id)
        app_client_id = Variable.get("knoema_app_client_id")
        app_secret = base64.b64encode(hmac.new(datetime.datetime.utcnow().strftime('%d-%m-%y-%H').encode(), str.encode('d1vGjfDRXkcgdA'), hashlib.sha1).digest()).decode('utf-8')
        # Request Headers
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',
            'accept-language': 'en-US,en;q=0.9',
            'Content-Type': 'application/json',
            'Authorization': 'Knoema {}:{}:1.2'.format(app_client_id, app_secret)
        }
        # Get Metadata
        meta_data_json = get_meta_information(META_URL, dataset_id, headers)
        
        # Overwrite the dataset id with a replacement dataset id
        if meta_data_json['replacementDataset'] != None:
            dataset_id = meta_data_json['replacementDataset']['id']

        # headers = {'Content-Type': 'application/json'}
        # Payload
        payload = {'Dataset': dataset_id}
        try:
            res = requests.post(url, headers=headers, data=json.dumps(payload))
        except requests.exceptions.Timeout:
            time.sleep(3)
            return get_raw_data(url, dataset_id)
        except requests.exceptions.RequestException as e:
            raise ValueError("Error: While fetching the raw data")
        
        if res.status_code == 200:
            raw_data_json = res.json()
        else:
            raise ValueError("Error: While fetching the raw data")
        
        formatted_raw_data_list = []
        for dat in raw_data_json['data']:
            formatted_raw_data_dict = {}
            for k, v in dat.items():
                if next((col.get('status') for col in raw_data_json['columns'] if col.get('id') == k)) == 'Dimension':
                    dim_name = next((col.get('name') for col in raw_data_json['columns'] if col.get('id') == k))
                    key_name = re.sub(r"[^a-zA-Z0-9]+", '_', dim_name.upper())
                    try:
                        for dim_key, dim_val in v.items():
                            if dim_val is not None:
                                formatted_raw_data_dict[str(key_name)+"_"+dim_key.strip().upper()] = str(dim_val.strip())
                    except Exception as e:
                        # print (e.message)
                        try:
                            formatted_raw_data_dict[str(key_name)] = str(v.strip())
                        except Exception as e:
                            formatted_raw_data_dict[str(key_name)] = str(v)
                elif next((col.get('status') for col in raw_data_json['columns'] if col.get('id') == k)) == 'Date':
                    dim_name = next((col.get('name') for col in raw_data_json['columns'] if col.get('id') == k))
                    key_name = re.sub(r"[^a-zA-Z0-9]+", '_', dim_name.upper())
                    if v == None:
                        formatted_raw_data_dict[str(key_name)] = ''    
                    else:
                        formatted_raw_data_dict[str(key_name)] = v['value'] if v['value'] == None else v['value'].strip()
                        for fk, fv in FREQ_MAPPING.items():
                            if fk == v['frequency']:
                                freq = fv if fv == None else fv.strip()
                                break
                            else:
                                freq = v['frequency']
                    formatted_raw_data_dict['FREQUENCY'] = freq
                elif next((col.get('status') for col in raw_data_json['columns'] if col.get('id') == k)) == 'Measure':
                    dim_name = next((col.get('name') for col in raw_data_json['columns'] if col.get('id') == k))
                    key_name = re.sub(r"[^a-zA-Z0-9]+", '_', dim_name.upper())
                    formatted_raw_data_dict[str(key_name)] = v
                elif next((col.get('status') for col in raw_data_json['columns'] if col.get('id') == k)) == 'Detail':
                    dim_name = next((col.get('name') for col in raw_data_json['columns'] if col.get('id') == k))
                    key_name = re.sub(r"[^a-zA-Z0-9]+", '_', dim_name.upper())
                    formatted_raw_data_dict[str(key_name)] = v if v == None else v.strip()
            formatted_raw_data_list.append(formatted_raw_data_dict)
        
        #print ("Starting to write data")
        # To CSV
        df = pd.DataFrame(formatted_raw_data_list)
        df.to_csv(os.path.join(DIR_PATH, '_Output', meta_data_json['id'] + '.csv'), sep=';', index=False)

        # Load Meta
        if 'freq' in locals():
            freq = freq
        else:
            freq = "Annual"
        return json.dumps({"meta_data_json": meta_data_json, "freq": freq})
    else:
        print (dataset_id + " - Already processed")
        raise ValueError("Error: dataset_id already processed")


def get_meta_information(url, dataset_id, headers):
    try:
        res = requests.get(urljoin(url, dataset_id), headers=headers)
    except requests.exceptions.Timeout:
        time.sleep(3)
        return get_meta_information(url, dataset_id)
    except requests.exceptions.RequestException as e:
        raise ValueError("Error: While getting the metadata")

    if res.status_code == 200:
        meta_data_json = res.json()
    else:
        raise ValueError("Error: While getting the metadata")
    return meta_data_json

def update_meta_info(datum):
    datum = json.loads(datum)
    meta_json = datum['meta_data_json']
    freq = datum['freq']
    id = meta_json['id']
    if meta_json['replacementDataset'] == None:
        replacement_id = None
    else:
        replacement_id = meta_json['replacementDataset']['id']
    source_dataset = meta_json['id'] + '.csv'
    title = meta_json['name']
    # if meta_json['description'] == None:
    #     description = None
    #     keywords = None
    # else:
    #     description = cleanhtml(meta_json['description'])
    #     description = re.sub(r"[^a-zA-Z0-9,]+", ' ', description)
    #     # Keywords
    #     r = Rake()
    #     r.extract_keywords_from_text(description)
    #     keywords =  ",".join(r.get_ranked_phrases())
    language = 'en'
    references = meta_json['ref']
    if meta_json['source'] == None:
        publisher = ""
    else:
        publisher = meta_json['source']['name']
    theme = next((col.get('displayValue') for col in meta_json['customMetadataFieldValues'] if col.get('id') == 'theme'))
    dcat_created = meta_json['publicationDate']
    dcat_issued = meta_json['lastUpdatedOn']

    meta_values = [{"KNOEMA_REPLACEMENT_ID": replacement_id}, {"KDS_SOURCE_DATASET": source_dataset}, 
                    {"KDS_TITLE": re.sub(r'\W+', ' ', title)}, 
                    # {"KDS_DESCRIPTION": description}, {"KDS_KEYWORDS": keywords}, 
                    {"KDS_LANGUAGE": language}, {"KDS_REFERENCES": references}, {"KDS_PUBLISHER": re.sub(r'\W+', ' ', publisher)}, {"KDS_THEME": theme}, 
                    {"KDS_DCAT_CREATED": dcat_created}, {"KDS_DCAT_ISSUED": dcat_issued}, {"KDS_DCAT_FREQ": freq},
                    {"NEXT_LOAD_DATE": datetime.datetime.utcnow()+datetime.timedelta(days=1)}, {"LAST_LOAD_DATE": datetime.datetime.utcnow()}]

    pg_db = PostgresHook(postgres_conn_id='pg_id_framework')
    src_conn = pg_db.get_conn()
    src_cursor = src_conn.cursor()
    for item in meta_values:
        for k, v in item.items():
            if v is not None:
                src_cursor.execute("UPDATE lkp.knoema SET {} = '"'{}'"' WHERE knoema_id = '"'{}'"'".format(k, v, id))
                src_conn.commit()
    src_cursor.close()
    src_conn.close()
    return str(id)

def cleanhtml(raw_html):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext

def get_datasets():
    pg_db = PostgresHook(postgres_conn_id='pg_id_framework')
    src_conn = pg_db.get_conn()
    src_cursor = src_conn.cursor()
    src_cursor.execute("SELECT knoema_id, kds_id, last_load_date, next_load_date FROM lkp.knoema where kds_id is not null group by knoema_id, kds_id, last_load_date, next_load_date")
    columns = [column[0] for column in src_cursor.description]
    datasets = [dict(zip(columns, row)) for row in src_cursor.fetchall()]
    src_cursor.close()
    src_conn.close()
    return datasets

def clear_folders(DIR_PATH, FOLDER_NAME, FILE_NAME):
    try:
        os.remove(os.path.join(DIR_PATH, FOLDER_NAME, FILE_NAME))
        return "Success"
    except ValueError:
        raise ValueError("Unable to remove the input file")

def push_to_aws(file_id):
    s3_hook = S3Hook(aws_conn_id='s3_conn')
    s3_hook.get_conn()
    local_path = os.path.join(DIR_PATH, '_Output', file_id+'.csv')
    s3_hook.load_file(filename=local_path, key=file_id+'.csv', bucket_name=Variable.get("s3_datasets_bucket_name"), replace=True)
    return "Sucess: Pushed the output"

def get_dataset_uid(dataset_id):
    try:
        res = requests.get(Variable.get("kds_api_url") + "catalog/datasets/" + dataset_id, auth=HTTPBasicAuth(Variable.get("kds_auth_email").strip(), Variable.get("kds_auth_password").strip()))
        if res.status_code == 200:
            return res.json()['dataset']['dataset_uid']
    except ValueError:
        raise ValueError("Error: While getting the dataset uid")

def update_dataset_metadata(uid, template, metadata_name, attribute, value):
    payload = '{"value": "'+value+'" , "override_remote_value": true}'
    try:
        res = requests.put(Variable.get("kds_api_management_url") + '/' + uid + '/' + template + '/' + metadata_name + '/' + attribute, data=payload, auth=HTTPBasicAuth(Variable.get("kds_auth_email").strip(), Variable.get("kds_auth_password").strip()))
        if res.status_code == 200:
            return res
    except ValueError:
        raise ValueError("Error while updating metadata on KDS")

def publish(dataset_id):
    dataset_uid = get_dataset_uid(dataset_id)
    update_metadata = update_dataset_metadata(dataset_uid, 'metadata', 'custom', 'last-checked-date', datetime.datetime.now().strftime("%Y-%m-%d"))
    if update_metadata.status_code == 200:
        try:
            response = requests.put(Variable.get("kds_api_management_url") + '/' + dataset_uid + '/publish/', auth=HTTPBasicAuth(Variable.get("kds_auth_email").strip(), Variable.get("kds_auth_password").strip()))
            return response.json()
        except ValueError:
            raise ValueError("Error while publishing the dataset_uid")

def each_id_dag(DIR_PATH, dataset, main_dag_id, start_date, schedule_interval):
    dag = DAG(
    '%s.%s' % (main_dag_id, str(dataset['knoema_id'])),
    schedule_interval=schedule_interval,
    start_date=start_date,
    )

    process_dataset = PythonOperator(
                    task_id = 'process_dataset',
                    python_callable=get_raw_data,
                    op_kwargs={'row': dataset},
                    start_date=airflow.utils.dates.days_ago(0),
                    dag=dag
                )
    
    update_meta = PythonOperator(
                    task_id = 'update_meta',
                    python_callable=update_meta_info,
                    op_kwargs={'datum': "{{ task_instance.xcom_pull(task_ids='process_dataset', key='return_value') }}"},
                    start_date=airflow.utils.dates.days_ago(0),
                    dag=dag
                )

    load_dataset_to_aws = PythonOperator(
                    task_id="push_to_aws", 
                    python_callable=push_to_aws,
                    op_kwargs={'file_id': "{{ task_instance.xcom_pull(task_ids='update_meta', key='return_value') }}"},
                    dag=dag
                )

    publish_dataset = PythonOperator(
                    task_id="PublishDataset", 
                    python_callable=publish,
                    op_kwargs={'dataset_id': dataset['kds_id']},
                    dag=dag
                )

    clear_outputs = PythonOperator(
                    task_id="ClearOutput", 
                    python_callable=clear_folders,
                    op_kwargs={'DIR_PATH': DIR_PATH, 'FOLDER_NAME': '_Output', 'FILE_NAME': "{{ task_instance.xcom_pull(task_ids='update_meta', key='return_value') }}"+'.csv'},
                    dag=dag
                )

    process_dataset >> update_meta >> load_dataset_to_aws >> publish_dataset >> clear_outputs
    return dag

default_args = {
    'owner': 'Anup Kumar',
    'depends_on_past': False,
    'start_date': airflow.utils.dates.days_ago(1),
    'email': [Variable.get("alert_email_to")],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': datetime.timedelta(minutes=10)
}


main_dag = DAG(
    'Knoema',
    default_args=default_args,
    description='Knoema data pipeline',
    schedule_interval='18 3 * * *',
)

start_task = DummyOperator(task_id="Start", dag=main_dag)
end_task = DummyOperator(task_id="End", dag=main_dag)

dataset_list = get_datasets()
for dataset in dataset_list:
    sub_dag = SubDagOperator(
            subdag=each_id_dag(DIR_PATH, dataset, 'Knoema', main_dag.default_args['start_date'], main_dag.schedule_interval),
            task_id=str(dataset['knoema_id']),
            dag=main_dag,
        )

    start_task >> sub_dag >> end_task