#-------------------------------------- Chnage Log -------------------------------------------
__author__ = "Anup Kumar"
__copyright__ = "Copyright 2019, KAPSARC EIM Project"
__credits__ = ["Anup Kumar"]
__license__ = "Prop"
__version__ = "1.0.0"
__maintainer__ = "Anup Kumar"
__email__ = "anup.kumar@kapsarc.org"
__status__ = "Production"
#------------------------------------ Change Details ------------------------------------------
# 02/08/2018 - Anup Kumar - Downloading the eia datasets
# 04/19/2018 - Anup Kumar - Added the multiple category and series capability in a more generic approach
# 08/01/2019 - Anup Kumar - Migration to airflow
# 09/10/2019 - Anup Kumar - Updated the FTP to AWS
#----------------------------------------------------------------------------------------------

import os, requests, time, csv, datetime, json, sys
from requests.auth import HTTPBasicAuth
import numpy as np
import pandas as pd
import airflow
from airflow import DAG
from airflow.models import Variable
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.hooks.postgres_hook import PostgresHook
from airflow.hooks.S3_hook import S3Hook
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.subdag_operator import SubDagOperator

DIR_PATH = os.path.abspath(os.path.dirname(__file__))

def raw_url(API_KEY, API_TYPE, CATEGORY_ID, SERIES_ID, QUERY_ID):
    if API_TYPE == 'series':
        e_url = 'http://api.eia.gov/' + API_TYPE + '/?api_key=' + API_KEY + '&' + QUERY_ID + '=' + SERIES_ID
    elif API_TYPE == 'category':
        e_url = 'http://api.eia.gov/' + API_TYPE + '/?api_key=' + API_KEY + '&' + QUERY_ID + '=' + CATEGORY_ID
    return e_url

def get_raw_byte(API_KEY, API_TYPE, CATEGORY_ID, SERIES_ID, QUERY_ID):
    EIA_URL = raw_url(API_KEY, API_TYPE, CATEGORY_ID, SERIES_ID, QUERY_ID)
    try:
        response = requests.get(EIA_URL)
        raw_byte = response.content
        return raw_byte
    except ValueError:
        raise ValueError("Error: Getting RAW Bytes")

def eia_data(DIR_PATH, API_KEY, KDS_ID, CATEGORY_ID, SERIES_ID, STATIC_HEADERS, DATA_HEADERS, API_TYPE, QUERY_ID):
    if API_TYPE == 'series':
        if len(SERIES_ID.split(',')) == 1:
            series_json = json.loads(get_raw_byte(API_KEY, API_TYPE, CATEGORY_ID, SERIES_ID, QUERY_ID))
            series_data = series_json['series'][0]['data']
            new_series_data = [{'PERIOD': k, 'VALUE': v} for k, v in series_data]
            df_series_data = pd.io.json.json_normalize(new_series_data)
            for stat_header in STATIC_HEADERS.split(","):
                for key, value in series_json['series'][0].items():
                    if key == stat_header:
                        df_series_data[key] = value
            df_series_data.to_csv(os.path.join(DIR_PATH, '_Output', KDS_ID + '.csv'),mode = 'w', index=False)
        elif len(SERIES_ID.split(',')) > 1:
            count = 0
            for ser_id in SERIES_ID.split(','):
                if count == 0:
                    series_json = json.loads(get_raw_byte(API_KEY, API_TYPE, CATEGORY_ID, ser_id, QUERY_ID))
                    series_data = series_json['series'][0]['data']
                    for key, value in series_json['series'][0].items():
                        if key == 'name':
                            name_head = value
                    new_series_data = [{'PERIOD': k, name_head: v} for k, v in series_data]
                    df_series_data = pd.io.json.json_normalize(new_series_data)
                    count = count + 1
                else:
                    series_json = json.loads(get_raw_byte(API_KEY, API_TYPE, CATEGORY_ID, ser_id, QUERY_ID))
                    series_data = series_json['series'][0]['data']
                    for key, value in series_json['series'][0].items():
                        if key == 'name':
                            name_head = value
                    new_series_data = [{'PERIOD': k, name_head: v} for k, v in series_data]
                    norm_df = pd.io.json.json_normalize(new_series_data)
                    df_series_data = pd.merge(df_series_data, norm_df, on='PERIOD', how='outer')
                    count = count + 1
            for stat_header in STATIC_HEADERS.split(","):
                for key, value in series_json['series'][0].items():
                    if key == stat_header:
                        df_series_data[key] = value
            df_series_data.to_csv(os.path.join(DIR_PATH, '_Output', KDS_ID + '.csv'),mode = 'w', index=False)
    elif API_TYPE == 'category':
        if len(CATEGORY_ID.split(',')) == 1:
            cat_json = json.loads(get_raw_byte(API_KEY, API_TYPE, CATEGORY_ID, SERIES_ID, QUERY_ID))
            df_cat_series_data = pd.DataFrame([])
            for child_series in cat_json['category']['childseries']:
                df_cat_series_data = df_cat_series_data.append(get_series_data(API_KEY, 'series', 'NO', child_series['series_id'], 'series_id', STATIC_HEADERS))
            df_cat_series_data = df_cleaning(df_cat_series_data, DIR_PATH)
            df_cat_series_data.to_csv(os.path.join(DIR_PATH, '_Output', KDS_ID + '.csv'),mode = 'w', index=False)
        elif len(CATEGORY_ID.split(',')) > 1:
            df_cat_series_data = pd.DataFrame([])
            for cat_id in CATEGORY_ID.split(','):
                cat_json = json.loads(get_raw_byte(API_KEY, API_TYPE, cat_id, SERIES_ID, QUERY_ID))
                for child_series in cat_json['category']['childseries']:
                    df_cat_series_data = df_cat_series_data.append(get_series_data(API_KEY, 'series', 'NO', child_series['series_id'], 'series_id', STATIC_HEADERS))
            df_cat_series_data = df_cleaning(df_cat_series_data, DIR_PATH)
            df_cat_series_data.to_csv(os.path.join(DIR_PATH, '_Output', KDS_ID + '.csv'),mode = 'w', index=False)
    return


def get_series_data(API_KEY, API_TYPE, CATEGORY_ID, SERIES_ID, QUERY_ID, STATIC_HEADERS):
    series_json = json.loads(get_raw_byte(API_KEY, API_TYPE, CATEGORY_ID, SERIES_ID, QUERY_ID))
    series_data = series_json['series'][0]['data']
    new_series_data = [{'PERIOD': k, 'VALUE': v} for k, v in series_data]
    df_series_data = pd.io.json.json_normalize(new_series_data)
    for stat_header in STATIC_HEADERS.split(","):
        for key, value in series_json['series'][0].items():
            if key == stat_header:
                df_series_data[key] = value
    return df_series_data

def df_cleaning(dat_frame, DIR_PATH):
    dat_frame = df_name_split(dat_frame)
    dat_frame = df_trimAllColumns(dat_frame)
    dat_frame = df_geopoints(dat_frame, DIR_PATH)
    dat_frame['VALUE'] = pd.to_numeric(dat_frame.VALUE.astype(str).str.replace(',',''), errors='coerce').fillna(0).astype(int)
    return dat_frame

def df_name_split(dat_frame):
    dat_frame['INDICATOR'] = dat_frame.name.str.split(',').str.get(0).map(lambda x: x.strip())
    dat_frame['COUNTRY'] = dat_frame.name.str.split(',').str.get(1).map(lambda x: x.strip())
    dat_frame['FREQ'] = dat_frame.name.str.split(',').str.get(2).map(lambda x: x.strip())
    dat_frame['GEOPOINT'] = dat_frame.name.str.split(',').str.get(1).map(lambda x: x.strip())
    dat_frame = dat_frame.rename(columns = {'units': 'UNITS'})
    dat_frame = dat_frame.drop(['name', 'f', 'geography'], axis=1, errors='ignore')
    return dat_frame

def df_trimAllColumns(dat_frame):
    trimStrings = lambda x: x.strip() if type(x) is str else x
    return dat_frame.applymap(trimStrings)

def df_geopoints(dat_frame, DIR_PATH):
    config_data = json.loads(open(os.path.join(DIR_PATH, '_Mapping', 'eia_config.json')).read())
    lat_long = {v['name']:[v['lat'], v['lng']] for v in config_data['countries'] if (('lat' in v) and ('lng' in v))}
    dat_frame.loc[:, 'GEOPOINT'] = dat_frame['GEOPOINT'].map(lat_long)
    return dat_frame

def get_datasets():
    pg_db = PostgresHook(postgres_conn_id='pg_id_framework')
    src_conn = pg_db.get_conn()
    src_cursor = src_conn.cursor()
    src_cursor.execute('SELECT kds_id, eia_category_id, eia_series_id, eia_output_static_headers, eia_output_data_headers, eia_api_type, eia_query_id FROM lkp.eia')
    columns = [column[0] for column in src_cursor.description]
    datasets = [dict(zip(columns, row)) for row in src_cursor.fetchall()]
    src_cursor.close()
    src_conn.close()
    return datasets

def clear_folders(DIR_PATH, FOLDER_NAME, FILE_NAME):
    try:
        os.remove(os.path.join(DIR_PATH, FOLDER_NAME, FILE_NAME))
        return "Success"
    except ValueError:
        raise ValueError("Unable to remove the input file")

def push_to_aws(kds_id):
    s3_hook = S3Hook(aws_conn_id='s3_conn')
    s3_hook.get_conn()
    local_path = os.path.join(DIR_PATH, '_Output', kds_id+'.csv')
    s3_hook.load_file(filename=local_path, key=kds_id+'.csv', bucket_name=Variable.get("s3_datasets_bucket_name"), replace=True)
    return "Sucess: Pushed the output"

def get_dataset_uid(dataset_id):
    try:
        res = requests.get(Variable.get("kds_api_url") + "catalog/datasets/" + dataset_id, auth=HTTPBasicAuth(Variable.get("kds_auth_email").strip(), Variable.get("kds_auth_password").strip()))
        if res.status_code == 200:
            return res.json()['dataset']['dataset_uid']
    except ValueError:
        raise ValueError("Error: While getting the dataset uid")

def update_dataset_metadata(uid, template, metadata_name, attribute, value):
    payload = '{"value": "'+value+'" , "override_remote_value": true}'
    try:
        res = requests.put(Variable.get("kds_api_management_url") + '/' + uid + '/' + template + '/' + metadata_name + '/' + attribute, data=payload, auth=HTTPBasicAuth(Variable.get("kds_auth_email").strip(), Variable.get("kds_auth_password").strip()))
        if res.status_code == 200:
            return res
    except ValueError:
        raise ValueError("Error while updating metadata on KDS")

def publish(dataset_id):
    dataset_uid = get_dataset_uid(dataset_id)
    update_metadata = update_dataset_metadata(dataset_uid, 'metadata', 'custom', 'last-checked-date', datetime.datetime.now().strftime("%Y-%m-%d"))
    if update_metadata.status_code == 200:
        try:
            response = requests.put(Variable.get("kds_api_management_url") + '/' + dataset_uid + '/publish/', auth=HTTPBasicAuth(Variable.get("kds_auth_email").strip(), Variable.get("kds_auth_password").strip()))
            return response.json()
        except ValueError:
            raise ValueError("Error while publishing the dataset_uid")

def each_id_dag(DIR_PATH, dataset, main_dag_id, start_date, schedule_interval):
    dag = DAG(
    '%s.%s' % (main_dag_id, str(dataset['kds_id'])),
    schedule_interval=schedule_interval,
    start_date=start_date,
    )

    process_dataset = PythonOperator(
                    task_id = 'process_dataset',
                    python_callable=eia_data,
                    op_kwargs={'DIR_PATH': DIR_PATH, 'API_KEY': Variable.get("kds_eia_api_key"), 'KDS_ID': dataset['kds_id'], 'CATEGORY_ID': dataset['eia_category_id'], 'SERIES_ID': dataset['eia_series_id'], 'STATIC_HEADERS': dataset['eia_output_static_headers'], 'DATA_HEADERS': dataset['eia_output_data_headers'], 'API_TYPE': dataset['eia_api_type'], 'QUERY_ID': dataset['eia_query_id']},
                    start_date=airflow.utils.dates.days_ago(0),
                    dag=dag
                )

    load_dataset_to_aws = PythonOperator(
                    task_id="push_to_aws", 
                    python_callable=push_to_aws,
                    op_kwargs={'kds_id': dataset['kds_id']},
                    dag=dag
                )

    publish_dataset = PythonOperator(
                    task_id="PublishDataset", 
                    python_callable=publish,
                    op_kwargs={'dataset_id': dataset['kds_id']},
                    dag=dag
                )

    clear_outputs = PythonOperator(
                    task_id="ClearOutput", 
                    python_callable=clear_folders,
                    op_kwargs={'DIR_PATH': DIR_PATH, 'FOLDER_NAME': '_Output', 'FILE_NAME': str(dataset['kds_id'])+'.csv'},
                    dag=dag
                )

    process_dataset >> load_dataset_to_aws >> publish_dataset >> clear_outputs
    return dag

default_args = {
    'owner': 'Anup Kumar',
    'depends_on_past': False,
    'start_date': airflow.utils.dates.days_ago(1),
    'email': [Variable.get("alert_email_to")],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': datetime.timedelta(minutes=10)
}

main_dag = DAG(
    'EIA',
    default_args=default_args,
    description='US Energy Information Administration',
    schedule_interval='15 9 * * *',
)

start_task = DummyOperator(task_id="Start", dag=main_dag)
end_task = DummyOperator(task_id="End", dag=main_dag)

datasets = get_datasets()
for dataset in datasets:
    sub_dag = SubDagOperator(
            subdag=each_id_dag(DIR_PATH, dataset, 'EIA', main_dag.default_args['start_date'], main_dag.schedule_interval),
            task_id=str(dataset['kds_id']),
            dag=main_dag,
        )

    start_task >> sub_dag >> end_task