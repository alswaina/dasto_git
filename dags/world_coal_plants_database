#-------------------------------------- Chnage Log -------------------------------------------
__author__ = "Anup Kumar"
__copyright__ = "Copyright 2017, KAPSARC EIM Project"
__credits__ = ["Anup Kumar"]
__license__ = "Prop"
__version__ = "1.0.0"
__maintainer__ = "Anup Kumar"
__email__ = "anup.kumar@kapsarc.org"
__status__ = "Production"
#------------------------------------ Change Details ------------------------------------------
# 10/15/2017 - Anup Kumar - World coal plants json download
# 08/06/2019 - Anup Kumar - Migrating to Airflow
# 09/10/2019 - Anup Kumar - Updated the FTP to AWS
#----------------------------------------------------------------------------------------------

import os, requests, time, csv, datetime, json, sys
from requests.auth import HTTPBasicAuth
import airflow
from airflow import DAG
from airflow.models import Variable
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.hooks.postgres_hook import PostgresHook
from airflow.hooks.S3_hook import S3Hook
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.subdag_operator import SubDagOperator

DIR_PATH = os.path.dirname(os.path.realpath(__file__))
FILE_NAME = '6636E1BA-F632-E711-80CF-0050569F708D.csv'


def download_file(DIR_PATH, URL):
    try:
        data_json = requests.get(URL).json()
        with open(os.path.join(DIR_PATH, '_Input', 'EC_Tracker.json'), 'w') as f_track:
            json.dump(data_json, f_track)
        return
    except ValueError:
        raise ValueError("Error: While downloading the end coal input file")

def data_processing(DIR_PATH, FILE_NAME):
    is_python3 = sys.version_info.major == 3
    if is_python3:
        unicode = str

    # Processing the file
    ec_json_data = json.loads(open(os.path.join(DIR_PATH, '_Input', 'EC_Tracker.json')).read())
    with open(os.path.join(DIR_PATH, '_Output', FILE_NAME),'w') as kds_end_coal:
        writer = csv.writer(kds_end_coal, delimiter=',', quoting=csv.QUOTE_NONNUMERIC)
        col_header = ['UNIT', 'PLANT', 'PARENT', 'URL', 'SPONSOR', 'CAPACITY_MW', 'STATUS', 'REGION', 'COUNTRY', 'SUBNATIONAL_UNIT', 'LATITUDE', 'LONGITUDE']
        writer.writerow(col_header)
        for row in ec_json_data:
            row_list = []
            row_list.append(row['unit'].title().strip())
            row_list.append(row['plant'].title().strip())
            row_list.append(row['parent'].title().strip())
            row_list.append(row['url'].strip())
            row_list.append(row['sponsor'].title().strip())
            row_list.append(row['capacity'])
            row_list.append(row['status'].title().strip())
            row_list.append(row['region'].title().strip())
            row_list.append(row['country'].title().strip())
            row_list.append(row['subnational'].title().strip())
            row_list.append(float(row['lat']))
            row_list.append(float(row['lng']))
            writer.writerow(row_list)
    return

def get_datasets():
    pg_db = PostgresHook(postgres_conn_id='pg_id_framework')
    src_conn = pg_db.get_conn()
    src_cursor = src_conn.cursor()
    src_cursor.execute("SELECT kds_id FROM kap.job_details where job_id = 3")
    columns = [column[0] for column in src_cursor.description]
    datasets = [dict(zip(columns, row)) for row in src_cursor.fetchall()]
    src_cursor.close()
    src_conn.close()
    return datasets

def clear_folders(DIR_PATH, FOLDER_NAME, FILE_NAME):
    try:
        os.remove(os.path.join(DIR_PATH, FOLDER_NAME, FILE_NAME))
        return "Success"
    except ValueError:
        raise ValueError("Unable to remove the input file")

def push_to_aws(FILE_NAME):
    s3_hook = S3Hook(aws_conn_id='s3_conn')
    s3_hook.get_conn()
    local_path = os.path.join(DIR_PATH, '_Output', FILE_NAME)
    s3_hook.load_file(filename=local_path, key=FILE_NAME, bucket_name=Variable.get("s3_datasets_bucket_name"), replace=True)
    return "Sucess: Pushed the output"

def get_dataset_uid(dataset_id):
    try:
        res = requests.get(Variable.get("kds_api_url") + "catalog/datasets/" + dataset_id, auth=HTTPBasicAuth(Variable.get("kds_auth_email").strip(), Variable.get("kds_auth_password").strip()))
        if res.status_code == 200:
            return res.json()['dataset']['dataset_uid']
    except ValueError:
        raise ValueError("Error: While getting the dataset uid")

def update_dataset_metadata(uid, template, metadata_name, attribute, value):
    payload = '{"value": "'+value+'" , "override_remote_value": true}'
    try:
        res = requests.put(Variable.get("kds_api_management_url") + '/' + uid + '/' + template + '/' + metadata_name + '/' + attribute, data=payload, auth=HTTPBasicAuth(Variable.get("kds_auth_email").strip(), Variable.get("kds_auth_password").strip()))
        if res.status_code == 200:
            return res
    except ValueError:
        raise ValueError("Error while updating metadata on KDS")

def publish(dataset_id):
    dataset_uid = get_dataset_uid(dataset_id)
    update_metadata = update_dataset_metadata(dataset_uid, 'metadata', 'custom', 'last-checked-date', datetime.datetime.now().strftime("%Y-%m-%d"))
    if update_metadata.status_code == 200:
        try:
            response = requests.put(Variable.get("kds_api_management_url") + '/' + dataset_uid + '/publish/', auth=HTTPBasicAuth(Variable.get("kds_auth_email").strip(), Variable.get("kds_auth_password").strip()))
            return response.json()
        except ValueError:
            raise ValueError("Error while publishing the dataset_uid")

def each_id_dag(DIR_PATH, dataset, main_dag_id, start_date, schedule_interval):
    dag = DAG(
    '%s.%s' % (main_dag_id, str(dataset['kds_id'])),
    schedule_interval=schedule_interval,
    start_date=start_date,
    )

    extract_source = PythonOperator(
                    task_id = 'extract',
                    python_callable=download_file,
                    op_kwargs={'DIR_PATH': DIR_PATH, 'URL': 'https://greeninfo-network.github.io/coal-tracker-client/data/trackers.json'},
                    start_date=airflow.utils.dates.days_ago(0),
                    dag=dag
                )

    process_dataset = PythonOperator(
                    task_id = 'process_dataset',
                    python_callable=data_processing,
                    op_kwargs={'DIR_PATH': DIR_PATH, 'FILE_NAME': FILE_NAME},
                    start_date=airflow.utils.dates.days_ago(0),
                    dag=dag
                )

    load_dataset_to_aws = PythonOperator(
                    task_id="push_to_aws", 
                    python_callable=push_to_aws,
                    op_kwargs={'FILE_NAME': FILE_NAME},
                    dag=dag
                )

    publish_dataset = PythonOperator(
                    task_id="PublishDataset", 
                    python_callable=publish,
                    op_kwargs={'dataset_id': dataset['kds_id']},
                    dag=dag
                )

    clear_outputs = PythonOperator(
                    task_id="ClearOutput", 
                    python_callable=clear_folders,
                    op_kwargs={'DIR_PATH': DIR_PATH, 'FOLDER_NAME': '_Output', 'FILE_NAME': FILE_NAME},
                    dag=dag
                )

    extract_source >> process_dataset >> load_dataset_to_aws >> publish_dataset >> clear_outputs
    return dag

default_args = {
    'owner': 'Anup Kumar',
    'depends_on_past': False,
    'start_date': airflow.utils.dates.days_ago(1),
    'email': [Variable.get("alert_email_to")],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': datetime.timedelta(minutes=10)
}

main_dag = DAG(
    'World_Coal_Plants_Database',
    default_args=default_args,
    description='Greeninfo Tracker - World coal plants database',
    schedule_interval='45 5 * * *',
)

start_task = DummyOperator(task_id="Start", dag=main_dag)
end_task = DummyOperator(task_id="End", dag=main_dag)

clear_inputs = PythonOperator(
                    task_id="ClearInput", 
                    python_callable=clear_folders,
                    op_kwargs={'DIR_PATH': DIR_PATH, 'FOLDER_NAME': '_Input', 'FILE_NAME': 'EC_Tracker.json'},
                    dag=main_dag
                )

datasets = get_datasets()
for dataset in datasets:
    sub_dag = SubDagOperator(
            subdag=each_id_dag(DIR_PATH, dataset, 'World_Coal_Plants_Database', main_dag.default_args['start_date'], main_dag.schedule_interval),
            task_id=str(dataset['kds_id']),
            dag=main_dag,
        )

    start_task >> sub_dag >> clear_inputs >> end_task